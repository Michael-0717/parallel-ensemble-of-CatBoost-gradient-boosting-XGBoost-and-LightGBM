# Параллельный ансамбль из CatBoost, градиентного бустинга, XGBoost и LightGBM
## Задание
Загрузите данные, приведите их к числовым, заполните пропуски, нормализуйте данные и оптимизируйте память.
Сформируйте параллельный ансамбль (стекинг) из CatBoost, градиентного бустинга, XGBoost и LightGBM. Используйте лучшие гиперпараметры, подобранные ранее, или найдите их через перекрестную проверку. Итоговое решение рассчитайте на основании самого точного предсказания класса у определенной модели ансамбля: выберите для каждого класса модель, которая предсказывает его лучше всего.
Проведите расчеты и выгрузите результат в виде submission.csv
Данные:
•	https://video.ittensive.com/machine-learning/prudential/train.csv.gz
•	https://video.ittensive.com/machine-learning/prudential/test.csv.gz
•	https://video.ittensive.com/machine-learning/prudential/sample_submission.csv.gz
Соревнование: https://www.kaggle.com/c/prudential-life-insurance-assessment/
© ITtensive, 2020
## Решение
1) Подключим библиотеки, загрузим данные.
- Посмотрим сводную информацию методом info():
в таблице 59381 строки (0 to 59380)
128 записей (типы данных: Числа с плавающей точкой – 18, Целые числа – 109, смешанные - 1)
объем памяти - 58.0+ MB.
- Посмотрим сводную информацию методом info()
в таблице 59381 строки (0 to 59380)
128 записей (типы данных: Числа с плавающей точкой – 18, Целые числа – 109, смешанные - 1)
объем памяти - 58.0+ MB.
- Прочитаем файл данных, в столбце «Product_Info_2» тип данных смешенный (буква-цифра).
2) Проведем нормализацию данных через предварительную обработку – вызовем функцию  preprocess по параметру «df»:
- Для предварительной обработки данных переведем смешенное значение параметров #"Product_Info_2" отдельно в числовое и буквенное значение 
- с помощью функции «to_numeric» преобразуем аргумент в числовой тип
- удалим из набора параметров исходную серию данных, чтобы ее не учитывать в модели.   
- в массиве данных «Product_Info_2_1» используя метод «unique()» используя уникальные элементы данных
- с помощью функции astype() данные преобразуем  в целочисленный массив «"int8"»
- удалим из набора параметров  данные «Product_Info_2_1», чтобы ее не учитывать в модели.      
- заменим нулевые значения на «-1»
- проведем предварительную обработку данных.
3) Выделим набор столбцов для расчета, проведем группировку данных по столбцам «Страховая история», «Застрахованная информация», «Медицинское ключевое слово», «Семейная история», «История болезни», «Информация о продукте».
Используем данные: рост (Ht), вес (Wt), возраст (Ins_Age), индекс массы тела (BMI).
Группируем данные, первый массив данных «columns» добавляет в конец второго массива.
4) Алгоритмы машинного обучения работают лучше или сходятся быстрее, когда данные примерно одинакового масштаба или близки к нормальному распределению.
- Проведем нормализацию данных через предварительную обработку.
- Зададим переменную «scaler» для модуля «preprocessing» функции «StandardScaler()»(стандартный масштабатор), проведем стандартное нормальное распределение (SND) (среднее значение = 0 и данные масштабируются до единичной дисперсии).
- Сформируем фрейм данных, с помощью метода «fit_transform» преобразуем его.
- Сформируем преобразованные столбцы по правилу исходных данных.
- Данные столбца "Response" («Ответ») преобразованных данных соответствуют столбцу исходных данных.
5) Оптимизация памяти:
- вызываем функцию «reduce_mem_usage» по параметру «df», которая перебирает все существующие столбцы. 
- суммируем использование столбцов и делим значение на 1024², мы получаем использование в МБ. Это помогает значительно сократить использование памяти.
- применим функцию «reduce_mem_usage» к фрейму данных «data_transformed».
- посмотрим сводную информацию методом info() - потребление памяти меньше на 40.49 Мб (минус 75.1 %).
6) Построение базовых моделей. Сформируем фрейм данных Х из преобразованных данных «data_transformed».
- модель «XGBClassifier»
XGBoost - более регуляризованная форма градиентного бустинга.
Основным преимуществом данной библиотеки является производительность и эффективная оптимизация вычислений (лучший результат с меньшей затратой ресурсов).
max_depth – максимальная глубина дерева
max_features  – количество признаков, учитываемых алгоритмом для построения разветвления  в дереве
n_estimators – число итераций в бустинге.
min_samples_leaf – минимальное число объектов в листе (узле).
С помощью метода  «fit» проведем обучение набора данных «model_xgb» на основе фрейма данных Х, исходного набора данных «data», где используем только столбец «Response».
-  модель «CatBoost»
CatBoost - это библиотека градиентного бустинга, созданная Яндексом. Она использует небрежные (oblivious) деревья решений, чтобы вырастить сбалансированное дерево. 
Iterations – количество итераций
learning_rate - скорость обучения алгоритма
random_seed - случайное начальное число
depth -  глубина дерева в ансамбле
l2_leaf_reg - коэффициент потерь
loss_function='MultiClass' – метрика обучения «MultiClass»
bootstrap_type - тип начальной загрузки
С помощью метода  «fit» проведем обучение набора данных «model_cb» на основе фрейма данных Х, исходного набора данных «data», где используем только столбец «Response» (метка “label”).
- Градиентный бустинг
Деревья для градиентного бустинга строятся последовательно для минимизации ошибки предыдущего дерева (или деревьев). При этом в самом дереве разбиение выполняется по минимизации информационных потерь, без учета сортировки исходных данных по количеству информации в них.
Построим ансамбль решающих деревьев, используя градиентный бустинг (GradientBoostingClassifier). 
Используем параметры: 
random_state – начальное значение
max_depth – максимальная глубина дерева
max_features - максимальное количество признаков, учитываемых алгоритмом для построения разветвления  в дереве
min_samples_leaf - минимальное количество выборок
n_estimators - количество оценщиков
С помощью метода  «fit» проведем обучение набора данных «model_gbc» на основе фрейма данных Х, исходного набора данных «data», где используем только столбец «Response».
- LightGBM - облегченный  градиентный бустинг
Основное отличие этого градиентного бустинга от предыдущих - использование сильно-разнородных (определяется разностью, гистограммой самих данных) экземпляров в выборке для формирования первоначального дерева: сначала рассматриваются все крайние, "плохие", случаи, а затем к ним "достраиваются" средние, "хорошие". Это позволяет еще быстрее минимизировать ошибку моделей.
Из дополнительных плюсов: алгоритм запускается сразу на всех ядрах процессора, это существенно ускоряет работу.
Используем параметры: 
random_state – начальное значение
max_depth – максимальная глубина дерева
min_child_samples - минимальное число элементов выборке в листе
num_leaves - число листьев в каждом дереве
n_estimators - количество оценщиков
С помощью метода  «fit» проведем обучение набора данных «model_lgb» на основе фрейма данных Х, исходного набора данных «data», где используем только столбец «Response».

Применим функцию «preprocess» к массиву  данных «data_test», необходимых для дальнейших расчетов.
С помощью функции « reduce_mem_usage»  уменьшим пространство, занимаемое данными в памяти, путем настройки типа данных.
Сформируем фрейм данных, с помощью метода «transform» преобразуем его.
Посмотрим сводную информацию методом info() - потребление памяти меньше на 16.34 Мб (минус 84.9 %)
7) Проведем расчет предсказаний для четырех моделей.
- Посмотрим предсказания для четырех моделей: 
Классы смещены на 1: начинаются от 0 и заканчиваются 7. Судя по рассчитанным матрицам ошибок, для 0, 1, 3, 4 и 6 классов точнее работает градиентный бустинг, для 2 - XGBoost, для 5 - LightGBM, для 7 - логистическая регрессия.
- Вызовим функцию "vote_class" по параметру "x", уточним параметры классов для моделей.
- преобразуем массив данных "data_test" по условиям функции "vote_class"
- просмотрим первые пять строк в Dataframe «data_test».
8) Формирование и выгрузка результатов.
- Загрузим примерный файл, заменим в нем результаты и сохраним.
- Число строк в файле будет равно размену набора данных + 1 заголовочная строка.
9) Рассчитаем точность классификации на обучающих данных:
Проверяем модель: xgb Максимальная оценка: 0.91923523748714
Проверяем модель: cb Максимальная оценка: 0.923490383367688
Проверяем модель: gbc Максимальная оценка: 0.9235305740712393
Проверяем модель: lgb Максимальная оценка: 0.9239003098498227.








